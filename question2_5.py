# -*- coding: utf-8 -*-
"""question2_5

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/18xEDsapQFVW6308Qzcwtgwf9V6t8TcjT
"""

#Importing our data that was scraped earlier
df=pd.read_csv('/content/drive/MyDrive/politics_onlinekhabar (1).csv')

df.head()

#USING GENSIM LIBRARY
import gensim
from gensim.parsing.preprocessing import remove_stopwords
def clean_sentence(sentence,stopwords=False):
    sentence=sentence.lower().strip()
    sentence=re.sub(r'[^0-9a-z\s]','',sentence)
    if stopwords:
        sentence=remove_stopwords(sentence)
    return sentence
    def get_cleaned_sentence(sentence,stopwords=False):
        sents=df[['contents']]
        cleaned_sentences=[]
        for i,row in df.iterrows():
            clean=clean_sentence(row['contents'],stopwords)
            cleaned_sentences.append(clean)
        return cleaned_sentences
sentences=get_cleaned_sentence(df,stopwords=False)

#Question No 2
# Process text for sentences and apply necessary NLP processings. You can use nltk,
# spacy or any NLP libraries.

import spacy
nlp=en_core_web_sm    #this code is colab specific ; Wehen on local host I use nlp=spacy.load('en_core_web_sm') or md or lg for medium and larger library

#Passing sentence thorugh nlp pipeline
sentences=nlp(str(sentences))

#Printing out the sentences
for sents in sentences.sents:
  print(sents)

# 3. Extract the subject, object and relationship from each sentence. Extracting entities with modifiers is a plus point.

i=0

for sents in (sentences.sents):
    
    i+=1
    print(f'Sentence number : #{i}"\n"{sents}')
    
    
    print("\n\nTokens : ")
    for token in sents:
        print (token.text,end="|" )
        
        
    print("\n\nObjects:")
    for token in sents:
        if token.dep_=='dobj':
            print(token.text,'->',token.dep_,'i.e',spacy.explain(token.dep_),end=" | ")
    print('\nsubjects:')
    for token in sents:
        if token.dep_=='nsubj':
            print(token.text,'->',token.dep_,'i.e',spacy.explain(token.dep_),end=" | ")
  
        
    print("\n\nParts of speech:")
    for token in sents:
        print (token.pos_,end="|" )
        
    print("\n\nDependencies: ")
    for token in sents:
        print (token.dep_,end="|" )
    
    print("\n\nEntities in the sentence are: ")
    for tokens in sents.ents:
        print(tokens.ents,tokens.label_,end="|")
    
    print("\nEntities explanation: ")
    for tokens in sents.ents:
        print(spacy.explain(tokens.label_),end='|')

    print('\n\n')

# 4:Build a directed graph from the above data with entities as a node and relationships as
# an edge. Label each node and edge with corresponding texts. You can use nexworkx,
# arongodb, neo4j or any graph/network library.

# Commented out IPython magic to ensure Python compatibility.
import networkx as nx
import re
from spacy.matcher import Matcher 
from spacy.tokens import Span 
import matplotlib.pyplot as plt
from tqdm import tqdm
pd.set_option('display.max_colwidth', 200)
# %matplotlib inline

def get_entities(sent):
  ## chunk 1
  ent1 = ""
  ent2 = ""

  prv_tok_dep = ""    # dependency tag of previous token in the sentence
  prv_tok_text = ""   # previous token in the sentence

  prefix = ""
  modifier = ""

  #############################################################
  
  for tok in nlp(sent):
    ## chunk 2
    # if token is a punctuation mark then move on to the next token
    if tok.dep_ != "punct":
      # check: token is a compound word or not
      if tok.dep_ == "compound":
        prefix = tok.text
        # if the previous word was also a 'compound' then add the current word to it
        if prv_tok_dep == "compound":
          prefix = prv_tok_text + " "+ tok.text
      
      # check: token is a modifier or not
      if tok.dep_.endswith("mod") == True:
        modifier = tok.text
        # if the previous word was also a 'compound' then add the current word to it
        if prv_tok_dep == "compound":
          modifier = prv_tok_text + " "+ tok.text
      
      ## chunk 3
      if tok.dep_.find("subj") == True:
        ent1 = modifier +" "+ prefix + " "+ tok.text
        prefix = ""
        modifier = ""
        prv_tok_dep = ""
        prv_tok_text = ""      

      ## chunk 4
      if tok.dep_.find("obj") == True:
        ent2 = modifier +" "+ prefix +" "+ tok.text
        
      ## chunk 5  
      # update variables
      prv_tok_dep = tok.dep_
      prv_tok_text = tok.text
  #############################################################

  return [ent1.strip(), ent2.strip()]

#Cheking if our function performs well
get_entities('Oli has resigned as the president')

#Gathering entity pairs
entity_pairs = []
for sents in (sentences.sents):
  sents=str(sents)
  entity_pairs.append(get_entities(sents))

entity_pairs

#getting relations
from spacy.matcher import Matcher
matcher=Matcher(nlp.vocab)
def get_relation(sent):

  doc = nlp(sent)

  # Matcher class object 
  matcher = Matcher(nlp.vocab)

  #define the pattern 
  pattern = [{'DEP':'ROOT'}, 
            {'DEP':'prep','OP':"?"},
            {'DEP':'agent','OP':"?"},  
            {'POS':'ADJ','OP':"?"}] 

  matcher.add("matching_1", None, pattern) 

  matches = matcher(doc)
  k = len(matches) - 1

  span = doc[matches[k][1]:matches[k][2]] 

  return(span.text)

relations=[]

for sents in sentences.sents:
  sents=str(sents)
  relations.append(get_relation(sents))

pd.Series(relations).value_counts()[:50]

#Building Knowledge graph

# extracting subject
source = [i[0] for i in entity_pairs]

# extracting object
target = [i[1] for i in entity_pairs]

kg_df = pd.DataFrame({'source':source, 'target':target, 'edge':relations})

# creating a directed-graph from a dataframe

#Here I have decided to use networkx ; On later part of the question I have used neo4j with Cypher
import networkx as nx 
import matplotlib.pyplot as plt


G=nx.from_pandas_edgelist(kg_df, "source", "target", 
                          edge_attr=True, create_using=nx.MultiDiGraph())

#Plotting the nw
plt.figure(figsize=(12,12))

pos = nx.spring_layout(G)
nx.draw(G, with_labels=True, node_color='skyblue', edge_cmap=plt.cm.Blues, pos = pos)
plt.show()


plt.savefig('/content/drive/MyDrive/chatbot/directed_graph_whole.png') #5. Save the graph db.

plt.savefig('/content/drive/MyDrive/chatbot/directed_graph.jpg')

G=nx.from_pandas_edgelist(kg_df[kg_df['edge']=="needs"], "source", "target", 
                          edge_attr=True, create_using=nx.MultiDiGraph())

plt.figure(figsize=(12,12))
pos = nx.spring_layout(G, k = 0.5) # k regulates the distance between nodes
nx.draw(G, with_labels=True, node_color='skyblue', node_size=1500, edge_cmap=plt.cm.Blues, pos = pos)
plt.show()

Question no :5. Save the graph db. # I have also saved the graph more elaborately in a json format and as an png while using neo4j for question no6

# Saving the graph db. 

plt.savefig('/content/drive/MyDrive/chatbot/needs_relation.png')